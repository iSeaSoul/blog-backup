<h3>Original Link: <a href=http://ryan-daily.diandian.com/machine-learning-raw-note>http://ryan-daily.diandian.com/machine-learning-raw-note</a></h3>
<article>
 <h3 data-postname="Pattern Translations">
  <a href="#" onclick="return false" style="cursor:default;">
   机器学习粗糙笔记
  </a>
 </h3>
 <p class="date ">
  26th of October 2014
 </p>
 <p class="date " style="margin-top:-1.2em;">
  <a href="/?tag=machine-learning" target="_blank">
   machine-learning
  </a>
  <a href="/?tag=reading-note" target="_blank">
   reading-note
  </a>
  <a href="/?tag=interview" target="_blank">
   interview
  </a>
 </p>
 <!-- 内容页面 全文 -->
 <p>
 </p>
 <p>
  为了准备明天面试，看了许多概念性的东西。先粗糙的总结一下，有空再来细化之。
 </p>
 <h3>
  聚类
 </h3>
 <p>
  聚类算法的目的是将相同的物品划分到一个类别中去。这些问题大多是非监督学习。常用的有K-means算法。算法是基于迭代的：
 </p>
 <ul class="edui-filter-disc">
  <li>
   随机生成一些初始点
  </li>
  <li>
   对每个点求出与K个中心点的距离并分配到最近的点上去
  </li>
  <li>
   分配完成后计算出新的中心点
  </li>
  <li>
   重复过程2和3直到中心点不在变动
  </li>
 </ul>
 <h3>
  贝叶斯
 </h3>
 <p>
  一般考虑朴素贝叶斯。认为每个属性是无关的。
 </p>
 <p>
  优点：简单，小规模数据表现好，适合增量训练
 </p>
 <p>
  缺点：独立假设，无法识别组合特征
 </p>
 <h3>
  决策树
 </h3>
 <p>
  决策树叶子节点是类别（或回归值），非叶子节点是条件。
 </p>
 <p>
  训练树的ID3过程是计算信息熵的增量。C4.5用的是信息增益率。
 </p>
 <p>
  优点：解释性很强，很容易处理表现变量之间的影响。
 </p>
 <p>
  缺点：overfitting（过度拟合）。
 </p>
 <h3>
  GBDT
 </h3>
 <p>
  为了改进决策树的过度拟合，出现了GBDT（Gradient Boosting Decision Tree / 迭代决策树）。
 </p>
 <p>
  它计算了每棵树的残差，然后综合多棵树来得到更有优秀的回归结果。
 </p>
 <p>
  注意这种方法不能用于分类，因为分类没有加减的说法。
 </p>
 <h3>
  AdaBoost
 </h3>
 <p>
  类似上述方法，AdaBoost也是一个将多个弱分类器的结果综合起来得到更好结果的手段。
 </p>
 <p>
  大致思路是迭代计算每个弱分类器的权重和训练数据的权重，稳定后得到每个弱分类器的权重，综合起来得到分类结果。
 </p>
 <p>
  优点：准确率较高，实现简单
 </p>
 <p>
  缺点：对非正常数据很敏感（为这种训练数据赋了过高权值）
 </p>
 <h3>
  KNN
 </h3>
 <p>
  KNN的思路很简单，就是找到最近的K个点，将其归为最多的类（如果是回归，就是平均值）。
 </p>
 <p>
  优点：简单，对非正常数据不敏感，精度高
 </p>
 <p>
  缺点：计算量大
 </p>
 <h3>
  Logistic回归
 </h3>
 <p>
  一种线性分类器，找到一个超平面，来划分数据集。
  <span class="text-img-holder">
   <img alt="Test" height="48" src="http://m3.img.srcdd.com/farm4/d/2013/0728/11/8BCE075CB31A20C54B9A5EF755D192C8_B500_900_124_48.PNG" width="124"/>
  </span>
  为映射函数。
 </p>
 <p>
  定义出cost函数后，可以用梯度下降法来进行训练。
 </p>
 <p>
  优点：简单，计算速度快
 </p>
 <p>
  缺点：准确度低，线性
 </p>
 <h3>
  SVM
 </h3>
 <p>
  <strike>
   我能说我没看懂吗
  </strike>
  ...咳咳，简单来说，原始的SVM一般用来做0/1分类，对于每一类都有一条分界线（超平面），建出两个平行的超平面，使得间隔最大。间隔越大，误分次数越少。
 </p>
 <p>
  核函数可以将非线性的问题转化为线性的。
 </p>
 <p>
  优点：适用于非线性分类，复杂度低
 </p>
 <p>
  缺点：依赖核函数选取
 </p>
 <h3>
  EM
 </h3>
 <p>
  其实EM算法就是一个迭代过程的高度概括...上面的K-means过程就是个EM过程，给出一个期望，不断迭代最大化之。
 </p>
 <h3>
  HMM
 </h3>
 <p>
  隐式马尔科夫模型是马尔科夫链的扩展，状态本身无法被观测到，能观测到的是这些状态的输出。
 </p>
 <p>
  学习的目标包括两个矩阵：隐含状态转移矩阵、隐含状态到观测状态的转移矩阵（混淆矩阵）。
 </p>
 <p>
  点A3kqT点Ja网
  <a href="http://www.diandian.com/?ref=crawler">
   点点网
  </a>
 </p>
 <p>
 </p>
 <div class="footnotes">
  <hr>
   <div id="notes" style="">
    <a name="jjlnotes">
    </a>
    <iframe allowtransparency="true" frameborder="0" height="0" id="diandian_comments" scrolling="no" src="http://www.diandian.com/n/common/comment?feedId=f183c550-5d27-11e4-a010-d4ae52a7bec4&amp;notesTextColor=%23131313&amp;notesLinkColor=%23131313&amp;notesBlockQuoteColor=&amp;notesBlockBgColor=%23ffffff&amp;notesBlockBorderColor=&amp;notesBlockBgOpacity=0&amp;notesOperationLinkColor=&amp;notesEnableBorderRadius=&amp;notesIframeId=" width="530">
    </iframe>
   </div>
  </hr>
 </div>
</article>
